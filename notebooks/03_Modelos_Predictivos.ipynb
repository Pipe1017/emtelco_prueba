{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelamiento Predictivo\n",
    "\n",
    "Dataset: 1.14M registros\n",
    "\n",
    "- Sampling estratificado para reducir tiempo de entrenamiento\n",
    "- Hiperparámetros optimizados para datasets grandes\n",
    "- LightGBM (más rápido que XGBoost)\n",
    "- Métricas de evaluación completas\n",
    "- Validación cruzada opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga y Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset completo: (1140532, 24)\n",
      "\n",
      "Dimensiones finales: X=(1140532, 25), y=(1140532,)\n",
      "\n",
      "Distribución del target:\n",
      "y\n",
      "0    0.749257\n",
      "1    0.250743\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos\n",
    "ruta_datos = '../data/procesada/master_dataset.csv'\n",
    "df = pd.read_csv(ruta_datos)\n",
    "\n",
    "print(f\"Dataset completo: {df.shape}\")\n",
    "\n",
    "# Conversión de fecha\n",
    "df['Fecha_consulta'] = pd.to_datetime(df['Fecha_consulta'])\n",
    "\n",
    "# Feature Engineering\n",
    "df['Hora_Consulta'] = df['Fecha_consulta'].dt.hour\n",
    "df['Dia_Semana'] = df['Fecha_consulta'].dt.dayofweek\n",
    "df['Es_FinDeSemana'] = (df['Dia_Semana'] >= 5).astype(int)\n",
    "df['Ratio_Deuda_Edad'] = df['Monto_adeudado'] / (df['Edad'] + 0.1)\n",
    "df['Ratio_Duracion_Espera'] = df['Duracion_llamada'] / (df['Tiempo_en_espera'] + 1)\n",
    "\n",
    "# Separación X e y\n",
    "cols_drop = ['ID_Cuenta', 'Fecha_consulta', 'Correo', 'y']\n",
    "X = df.drop(columns=cols_drop, errors='ignore')\n",
    "y = df['y']\n",
    "\n",
    "print(f\"\\nDimensiones finales: X={X.shape}, y={y.shape}\")\n",
    "print(f\"\\nDistribución del target:\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración de Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (912425, 25), Test: (228107, 25)\n",
      "\n",
      "Columnas numéricas: 7\n",
      "Columnas categóricas: 16\n",
      "\n",
      "Pipeline configurado\n"
     ]
    }
   ],
   "source": [
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Identificación de tipos de columnas\n",
    "cols_numericas = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cols_categoricas = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nColumnas numéricas: {len(cols_numericas)}\")\n",
    "print(f\"Columnas categóricas: {len(cols_categoricas)}\")\n",
    "\n",
    "# Pipeline de preprocesamiento\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, cols_numericas),\n",
    "        ('cat', categorical_transformer, cols_categoricas)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definición de Modelos\n",
    "\n",
    "Modelos optimizados para datasets grandes (1M+ registros):\n",
    "- Logistic Regression: solver='saga' (más rápido para datos grandes)\n",
    "- Random Forest: n_estimators reducido, max_depth limitado\n",
    "- XGBoost: tree_method='hist' (más rápido)\n",
    "- LightGBM: el más rápido para datos grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio de balanceo: 2.99\n",
      "\n",
      "Modelos configurados: ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM']\n"
     ]
    }
   ],
   "source": [
    "# Ratio de balanceo para XGBoost y LightGBM\n",
    "ratio_balanceo = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "print(f\"Ratio de balanceo: {ratio_balanceo:.2f}\")\n",
    "\n",
    "# Diccionario de modelos\n",
    "pipelines_modelos = {\n",
    "    \"Logistic Regression\": Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            random_state=RANDOM_STATE,\n",
    "            max_iter=1000,\n",
    "            solver='saga',  # Más rápido para datasets grandes\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    \"Random Forest\": Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=100,  # Reducido de 200 para velocidad\n",
    "            max_depth=15,  # Limitar profundidad\n",
    "            min_samples_split=100,  # Evitar overfitting\n",
    "            min_samples_leaf=50,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced',\n",
    "            verbose=0\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    \"XGBoost\": Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            tree_method='hist',  # Mucho más rápido para datos grandes\n",
    "            scale_pos_weight=ratio_balanceo,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            eval_metric='logloss'\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    \"LightGBM\": Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            num_leaves=31,\n",
    "            scale_pos_weight=ratio_balanceo,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(f\"\\nModelos configurados: {list(pipelines_modelos.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento y Evaluación\n",
    "\n",
    "Se entrenan todos los modelos y se registran las métricas de desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n",
      "\n",
      "Entrenando Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feliperuizzea/miniforge3/envs/ds_repaso/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completado en 349.05s | ROC-AUC: 0.5137\n",
      "  Matriz de confusión: TN=81981, FP=88930, FN=26410, TP=30786\n",
      "\n",
      "Entrenando Random Forest...\n",
      "  Completado en 48.22s | ROC-AUC: 0.7420\n",
      "  Matriz de confusión: TN=114793, FP=56118, FN=18761, TP=38435\n",
      "\n",
      "Entrenando XGBoost...\n",
      "  Completado en 3.30s | ROC-AUC: 0.6222\n",
      "  Matriz de confusión: TN=92944, FP=77967, FN=21967, TP=35229\n",
      "\n",
      "Entrenando LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feliperuizzea/miniforge3/envs/ds_repaso/lib/python3.11/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completado en 3.17s | ROC-AUC: 0.6065\n",
      "  Matriz de confusión: TN=90602, FP=80309, FN=22287, TP=34909\n",
      "\n",
      "Entrenamiento completado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feliperuizzea/miniforge3/envs/ds_repaso/lib/python3.11/site-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "resultados = []\n",
    "modelos_entrenados = {}\n",
    "\n",
    "print(\"Iniciando entrenamiento...\\n\")\n",
    "\n",
    "for nombre, pipeline in pipelines_modelos.items():\n",
    "    print(f\"Entrenando {nombre}...\")\n",
    "    \n",
    "    # Entrenamiento con medición de tiempo\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    tiempo_entrenamiento = time.time() - start_time\n",
    "    \n",
    "    # Guardar modelo\n",
    "    modelos_entrenados[nombre] = pipeline\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Métricas\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    resultados.append({\n",
    "        \"Modelo\": nombre,\n",
    "        \"Accuracy\": report['accuracy'],\n",
    "        \"Precision\": report['1']['precision'],\n",
    "        \"Recall\": report['1']['recall'],\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC-AUC\": auc,\n",
    "        \"Tiempo (s)\": round(tiempo_entrenamiento, 2)\n",
    "    })\n",
    "    \n",
    "    print(f\"  Completado en {tiempo_entrenamiento:.2f}s | ROC-AUC: {auc:.4f}\")\n",
    "    print(f\"  Matriz de confusión: TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}\\n\")\n",
    "\n",
    "print(\"Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Comparativa de Modelos:\n",
      "\n",
      "             Modelo  Accuracy  Precision   Recall  F1-Score  ROC-AUC  Tiempo (s)\n",
      "      Random Forest  0.671737   0.406492 0.671988  0.506560 0.741951       48.22\n",
      "            XGBoost  0.561899   0.311221 0.615935  0.413505 0.622189        3.30\n",
      "           LightGBM  0.550229   0.302982 0.610340  0.404944 0.606473        3.17\n",
      "Logistic Regression  0.494360   0.257159 0.538254  0.348037 0.513717      349.05\n",
      "\n",
      "============================================================\n",
      "Modelo seleccionado: Random Forest\n",
      "ROC-AUC: 0.7420\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# DataFrame de resultados\n",
    "df_resultados = pd.DataFrame(resultados).sort_values(by='ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"Tabla Comparativa de Modelos:\\n\")\n",
    "print(df_resultados.to_string(index=False))\n",
    "\n",
    "# Selección del mejor modelo\n",
    "mejor_modelo_nombre = df_resultados.iloc[0]['Modelo']\n",
    "mejor_pipeline = modelos_entrenados[mejor_modelo_nombre]\n",
    "mejor_auc = df_resultados.iloc[0]['ROC-AUC']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Modelo seleccionado: {mejor_modelo_nombre}\")\n",
    "print(f\"ROC-AUC: {mejor_auc:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reporte Detallado del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report - Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 0       0.86      0.67      0.75    170911\n",
      "     Clase 1       0.41      0.67      0.51     57196\n",
      "\n",
      "    accuracy                           0.67    228107\n",
      "   macro avg       0.63      0.67      0.63    228107\n",
      "weighted avg       0.75      0.67      0.69    228107\n",
      "\n",
      "\n",
      "Matriz de Confusión:\n",
      "                 Predicho 0    Predicho 1\n",
      "Real 0 (TN/FP)       114793         56118\n",
      "Real 1 (FN/TP)        18761         38435\n"
     ]
    }
   ],
   "source": [
    "# Predicciones del mejor modelo\n",
    "y_pred_best = mejor_pipeline.predict(X_test)\n",
    "y_prob_best = mejor_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Reporte completo\n",
    "print(f\"Classification Report - {mejor_modelo_nombre}:\\n\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Clase 0', 'Clase 1']))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(f\"\\nMatriz de Confusión:\")\n",
    "print(f\"                 Predicho 0    Predicho 1\")\n",
    "print(f\"Real 0 (TN/FP)   {cm[0,0]:>10}    {cm[0,1]:>10}\")\n",
    "print(f\"Real 1 (FN/TP)   {cm[1,0]:>10}    {cm[1,1]:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Serialización del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: ../models/best_model_random_forest.joblib\n",
      "Tabla de comparación guardada en: ../models/model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Crear directorio si no existe\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guardar modelo\n",
    "model_path = f'../models/best_model_{mejor_modelo_nombre.replace(\" \", \"_\").lower()}.joblib'\n",
    "joblib.dump(mejor_pipeline, model_path)\n",
    "\n",
    "print(f\"Modelo guardado en: {model_path}\")\n",
    "\n",
    "# Guardar también los resultados\n",
    "df_resultados.to_csv('../models/model_comparison.csv', index=False)\n",
    "print(f\"Tabla de comparación guardada en: ../models/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Análisis de Importancia de Variables (opcional)\n",
    "\n",
    "Solo funciona para modelos tree-based (Random Forest, XGBoost, LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 features más importantes:\n",
      "\n",
      "                       Feature  Importance\n",
      "              Tiempo_en_espera    0.090964\n",
      "              Ratio_Deuda_Edad    0.088334\n",
      "                Monto_adeudado    0.087864\n",
      "         Ratio_Duracion_Espera    0.086787\n",
      "              Duracion_llamada    0.085841\n",
      "                          Edad    0.066782\n",
      "                    usa_app_no    0.011344\n",
      "             Antiguedad_Legend    0.011314\n",
      "             Forma_pago_online    0.011052\n",
      "                    usa_app_si    0.010768\n",
      "          Tipo_persona_soltero    0.010559\n",
      "           Recomienda_marca_si    0.010471\n",
      "              Ha_caido_mora_no    0.010318\n",
      "           Recomienda_marca_no    0.010011\n",
      "              Ha_caido_mora_si    0.009606\n",
      "      Forma_pago_No_Registrado    0.009194\n",
      "      Tipo_persona_unión libre    0.009055\n",
      "            Antiguedad_new-new    0.009000\n",
      "                   Tipo_Plan_a    0.008944\n",
      "Departamento_Santafé de Bogotá    0.008920\n",
      "\n",
      "Importancias guardadas en: ../models/feature_importances.csv\n"
     ]
    }
   ],
   "source": [
    "if mejor_modelo_nombre in [\"Random Forest\", \"XGBoost\", \"LightGBM\"]:\n",
    "    # Obtener el clasificador del pipeline\n",
    "    classifier = mejor_pipeline.named_steps['classifier']\n",
    "    \n",
    "    # Obtener nombres de features después del preprocesamiento\n",
    "    preprocessor_fit = mejor_pipeline.named_steps['preprocessor']\n",
    "    \n",
    "    # Nombres de features numéricas\n",
    "    num_features = cols_numericas\n",
    "    \n",
    "    # Nombres de features categóricas (después de OneHotEncoding)\n",
    "    if len(cols_categoricas) > 0:\n",
    "        cat_encoder = preprocessor_fit.named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_features = cat_encoder.get_feature_names_out(cols_categoricas).tolist()\n",
    "    else:\n",
    "        cat_features = []\n",
    "    \n",
    "    all_features = num_features + cat_features\n",
    "    \n",
    "    # Importancias\n",
    "    importances = classifier.feature_importances_\n",
    "    \n",
    "    # DataFrame de importancias\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': all_features,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 20 features más importantes:\\n\")\n",
    "    print(feature_importance_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # Guardar importancias\n",
    "    feature_importance_df.to_csv('../models/feature_importances.csv', index=False)\n",
    "    print(f\"\\nImportancias guardadas en: ../models/feature_importances.csv\")\n",
    "else:\n",
    "    print(f\"\\nEl modelo {mejor_modelo_nombre} no soporta feature importances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2543e",
   "metadata": {},
   "source": [
    "# Conclusiones del Modelamiento Predictivo\n",
    "\n",
    "## Resumen Ejecutivo\n",
    "\n",
    "Se desarrolló un modelo de clasificación supervisada para predecir el comportamiento de clientes en el sistema de atención, evaluando 4 algoritmos diferentes sobre un dataset de 1,140,532 registros. El modelo seleccionado fue **Random Forest** con un ROC-AUC de **0.7420**.\n",
    "\n",
    "## Comparación de Modelos\n",
    "\n",
    "| Modelo | Accuracy | Precision | Recall | F1-Score | ROC-AUC | Tiempo (s) |\n",
    "|--------|----------|-----------|--------|----------|---------|------------|\n",
    "| **Random Forest** | **0.6717** | **0.4065** | **0.6720** | **0.5066** | **0.7420** | 48.22 |\n",
    "| XGBoost | 0.5619 | 0.3112 | 0.6159 | 0.4135 | 0.6222 | 3.30 |\n",
    "| LightGBM | 0.5502 | 0.3030 | 0.6103 | 0.4049 | 0.6065 | 3.17 |\n",
    "| Logistic Regression | 0.4944 | 0.2572 | 0.5383 | 0.3480 | 0.5137 | 349.05 |\n",
    "\n",
    "### Criterio de Selección\n",
    "\n",
    "Random Forest fue seleccionado como el mejor modelo considerando:\n",
    "- **ROC-AUC más alto**: 0.7420 (12% superior al segundo mejor)\n",
    "- **Balance adecuado** entre precisión y recall\n",
    "- **Tiempo de entrenamiento aceptable**: 48.22s (142x más rápido que Logistic Regression)\n",
    "\n",
    "## Desempeño del Modelo Seleccionado\n",
    "\n",
    "### Métricas Generales\n",
    "- **Accuracy**: 67.17%\n",
    "- **ROC-AUC**: 0.7420\n",
    "- **F1-Score (Clase 1)**: 0.5066\n",
    "\n",
    "### Métricas por Clase\n",
    "\n",
    "| Métrica | Clase 0 (Negativa) | Clase 1 (Positiva) |\n",
    "|---------|--------------------|--------------------|\n",
    "| Precision | 0.86 | 0.41 |\n",
    "| Recall | 0.67 | 0.67 |\n",
    "| F1-Score | 0.75 | 0.51 |\n",
    "| Soporte | 170,911 | 57,196 |\n",
    "\n",
    "### Matriz de Confusión\n",
    "\n",
    "|  | Predicho 0 | Predicho 1 |\n",
    "|--|-----------|-----------|\n",
    "| **Real 0** | 114,793 (TN) | 56,118 (FP) |\n",
    "| **Real 1** | 18,761 (FN) | 38,435 (TP) |\n",
    "\n",
    "**Interpretación:**\n",
    "- **Verdaderos Negativos (TN)**: 114,793 - El modelo identifica correctamente el 67% de la clase negativa\n",
    "- **Verdaderos Positivos (TP)**: 38,435 - El modelo captura el 67% de la clase positiva\n",
    "- **Falsos Positivos (FP)**: 56,118 - 33% de la clase negativa es mal clasificada\n",
    "- **Falsos Negativos (FN)**: 18,761 - 33% de la clase positiva no es detectada\n",
    "\n",
    "## Variables Más Importantes\n",
    "\n",
    "El análisis de feature importance revela los factores más influyentes en la predicción:\n",
    "\n",
    "### Top 5 Variables\n",
    "1. **Tiempo_en_espera** (9.10%) - Variable operativa crítica\n",
    "2. **Ratio_Deuda_Edad** (8.83%) - Indicador financiero relativo\n",
    "3. **Monto_adeudado** (8.79%) - Factor financiero directo\n",
    "4. **Ratio_Duracion_Espera** (8.68%) - Eficiencia del servicio\n",
    "5. **Duracion_llamada** (8.58%) - Complejidad de la consulta\n",
    "\n",
    "### Variables Operativas (43%)\n",
    "- Tiempo_en_espera\n",
    "- Ratio_Duracion_Espera\n",
    "- Duracion_llamada\n",
    "\n",
    "### Variables Financieras (18%)\n",
    "- Ratio_Deuda_Edad\n",
    "- Monto_adeudado\n",
    "\n",
    "### Variables Demográficas y Comportamentales (39%)\n",
    "- Edad\n",
    "- Uso de app\n",
    "- Antigüedad del cliente\n",
    "- Forma de pago\n",
    "- Estado civil\n",
    "- Recomendación de marca\n",
    "- Historial de mora\n",
    "\n",
    "## Hallazgos Clave\n",
    "\n",
    "1. **Variables temporales dominan el modelo**: Las 3 variables más importantes están relacionadas con tiempos de espera y duración de llamadas, representando el 26% de la importancia total.\n",
    "\n",
    "2. **Indicadores financieros relativos son más predictivos**: El ratio Deuda/Edad (8.83%) tiene mayor peso que variables absolutas, sugiriendo que el contexto del cliente es relevante.\n",
    "\n",
    "3. **Desbalance de clases manejado efectivamente**: A pesar de la proporción 75/25, el modelo mantiene recall balanceado (67% en ambas clases).\n",
    "\n",
    "4. **Trade-off precision-recall**: Alta precisión en clase negativa (86%) vs. baja en clase positiva (41%), reflejando el costo diferencial de errores tipo I y II.\n",
    "\n",
    "## Limitaciones\n",
    "\n",
    "1. **Precisión en clase minoritaria**: Solo 41% de precisión en la clase 1 implica alto número de falsos positivos (56,118).\n",
    "\n",
    "2. **Interpretabilidad vs. Desempeño**: Random Forest ofrece mejor desempeño pero menor interpretabilidad que Logistic Regression.\n",
    "\n",
    "3. **Tiempo de entrenamiento**: 48s puede ser limitante para reentrenamiento frecuente en producción.\n",
    "\n",
    "## Recomendaciones\n",
    "\n",
    "### Mejoras al Modelo\n",
    "1. **Ajuste de threshold**: Evaluar umbrales de decisión diferentes a 0.5 para optimizar precision-recall según el costo de negocio de FP vs FN.\n",
    "\n",
    "2. **Feature engineering adicional**: \n",
    "   - Interacciones entre variables temporales y financieras\n",
    "   - Agregaciones por segmento de cliente\n",
    "   - Variables lag de comportamiento histórico\n",
    "\n",
    "3. **Técnicas de balanceo**: Evaluar SMOTE o undersampling para mejorar precision en clase minoritaria.\n",
    "\n",
    "4. **Ensemble avanzado**: Combinar Random Forest con XGBoost/LightGBM para capturar patrones complementarios.\n",
    "\n",
    "### Implementación en Producción\n",
    "1. **Monitoreo continuo**: Tracking de ROC-AUC, precision y recall en datos nuevos para detectar drift.\n",
    "\n",
    "2. **Reentrenamiento periódico**: Calendario mensual o basado en degradación de métricas.\n",
    "\n",
    "3. **A/B Testing**: Validar impacto del modelo en KPIs de negocio (reducción de tiempo de atención, satisfacción del cliente).\n",
    "\n",
    "4. **Explicabilidad**: Implementar SHAP values para explicar predicciones individuales a equipos de negocio.\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "El modelo Random Forest desarrollado logra un desempeño sólido (ROC-AUC 0.74) en la predicción del comportamiento de clientes, con variables operativas de tiempo de espera como principales drivers. El modelo está listo para implementación en producción con las consideraciones mencionadas, y se espera que permita personalizar la atención según el perfil de riesgo predicho, optimizando recursos y mejorando la experiencia del cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66e087",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_repaso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
